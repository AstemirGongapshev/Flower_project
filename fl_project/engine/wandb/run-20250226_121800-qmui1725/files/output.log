[I 2025-02-26 12:18:00,792] A new study created in memory with name: no-name-3da08241-0f25-4179-a553-10d56d8c73f9
[I 2025-02-26 12:18:02,089] Trial 0 finished with value: 0.6547633482516926 and parameters: {'model': 'MLPModel', 'optimizer': 'AdamW', 'learning_rate': 0.00041741103145554977, 'num_epochs': 30}. Best is trial 0 with value: 0.6547633482516926.
[I 2025-02-26 12:18:02,642] Trial 1 finished with value: 0.36001324735821977 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 0.0001376046224281084, 'num_epochs': 10}. Best is trial 0 with value: 0.6547633482516926.
[I 2025-02-26 12:18:03,745] Trial 2 finished with value: 0.6744778423455787 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 0.0002586666277297009, 'num_epochs': 22}. Best is trial 2 with value: 0.6744778423455787.
[I 2025-02-26 12:18:04,949] Trial 3 finished with value: 0.6581086681518669 and parameters: {'model': 'MLPModel', 'optimizer': 'RMSProp', 'learning_rate': 5.411777613634192e-05, 'num_epochs': 21}. Best is trial 2 with value: 0.6744778423455787.
[I 2025-02-26 12:18:05,246] Trial 4 finished with value: 0.47592220253618134 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'Adam', 'learning_rate': 9.113824426617919e-05, 'num_epochs': 3}. Best is trial 2 with value: 0.6744778423455787.
[I 2025-02-26 12:18:06,366] Trial 5 finished with value: 0.5327329191422516 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.00039014249040410817, 'num_epochs': 29}. Best is trial 2 with value: 0.6744778423455787.
[I 2025-02-26 12:18:07,599] Trial 6 finished with value: 0.6110932147103589 and parameters: {'model': 'MLPModel', 'optimizer': 'Adam', 'learning_rate': 0.00018671748438472326, 'num_epochs': 30}. Best is trial 2 with value: 0.6744778423455787.
[I 2025-02-26 12:18:07,953] Trial 7 finished with value: 0.45765098278386357 and parameters: {'model': 'MLPModel', 'optimizer': 'Adam', 'learning_rate': 3.410942900749475e-05, 'num_epochs': 5}. Best is trial 2 with value: 0.6744778423455787.
[I 2025-02-26 12:18:08,462] Trial 8 finished with value: 0.5995424956068358 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 0.009743208801387801, 'num_epochs': 9}. Best is trial 2 with value: 0.6744778423455787.
[I 2025-02-26 12:18:08,831] Trial 9 finished with value: 0.6787153682023588 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 0.00858435022663805, 'num_epochs': 6}. Best is trial 9 with value: 0.6787153682023588.
[I 2025-02-26 12:18:09,575] Trial 10 finished with value: 0.6660435099708089 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'AdamW', 'learning_rate': 0.005453081202639944, 'num_epochs': 14}. Best is trial 9 with value: 0.6787153682023588.
[I 2025-02-26 12:18:10,510] Trial 11 finished with value: 0.685598746206315 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 0.0015712932564524054, 'num_epochs': 21}. Best is trial 11 with value: 0.685598746206315.
[I 2025-02-26 12:18:11,306] Trial 12 finished with value: 0.7181950293450702 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 0.0021942798593300793, 'num_epochs': 19}. Best is trial 12 with value: 0.7181950293450702.
[I 2025-02-26 12:18:12,065] Trial 13 finished with value: 0.6164853600403936 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 0.001751354435586973, 'num_epochs': 21}. Best is trial 12 with value: 0.7181950293450702.
[I 2025-02-26 12:18:12,821] Trial 14 finished with value: 0.6171739693030478 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 0.0014700132011713927, 'num_epochs': 16}. Best is trial 12 with value: 0.7181950293450702.
[I 2025-02-26 12:18:13,662] Trial 15 finished with value: 0.6957324323199345 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 0.0014639249242627847, 'num_epochs': 25}. Best is trial 12 with value: 0.7181950293450702.
[I 2025-02-26 12:18:14,576] Trial 16 finished with value: 0.7012340674276055 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 0.0033752071713274764, 'num_epochs': 25}. Best is trial 12 with value: 0.7181950293450702.
[I 2025-02-26 12:18:15,407] Trial 17 finished with value: 0.6570526799659044 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.003067840861896122, 'num_epochs': 25}. Best is trial 12 with value: 0.7181950293450702.
[I 2025-02-26 12:18:16,063] Trial 18 finished with value: 0.3987599604027052 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'AdamW', 'learning_rate': 1.2858690645042558e-05, 'num_epochs': 17}. Best is trial 12 with value: 0.7181950293450702.
[I 2025-02-26 12:18:16,963] Trial 19 finished with value: 0.6316890562705069 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 0.0006773356503316678, 'num_epochs': 26}. Best is trial 12 with value: 0.7181950293450702.
