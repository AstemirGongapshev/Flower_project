[I 2025-02-26 12:17:06,153] A new study created in memory with name: no-name-8a922c0f-3a03-4e20-b6c6-8255ab62a3c0
[I 2025-02-26 12:17:07,307] Trial 0 finished with value: 0.46026878383098396 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'AdamW', 'learning_rate': 0.0002478548154200101, 'num_epochs': 18}. Best is trial 0 with value: 0.46026878383098396.
[I 2025-02-26 12:17:08,941] Trial 1 finished with value: 0.7019344400547992 and parameters: {'model': 'MLPModel', 'optimizer': 'RMSProp', 'learning_rate': 0.00025000120625943945, 'num_epochs': 22}. Best is trial 1 with value: 0.7019344400547992.
[I 2025-02-26 12:17:10,063] Trial 2 finished with value: 0.5453351020607605 and parameters: {'model': 'MLPModel', 'optimizer': 'AdamW', 'learning_rate': 4.5260733023626455e-05, 'num_epochs': 14}. Best is trial 1 with value: 0.7019344400547992.
[I 2025-02-26 12:17:10,750] Trial 3 finished with value: 0.6912876902724215 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'AdamW', 'learning_rate': 0.004062594804795973, 'num_epochs': 9}. Best is trial 1 with value: 0.7019344400547992.
[I 2025-02-26 12:17:11,242] Trial 4 finished with value: 0.593650859720973 and parameters: {'model': 'MLPModel', 'optimizer': 'RMSProp', 'learning_rate': 6.245840503568521e-05, 'num_epochs': 3}. Best is trial 1 with value: 0.7019344400547992.
[I 2025-02-26 12:17:12,860] Trial 5 finished with value: 0.7278563711287219 and parameters: {'model': 'MLPModel', 'optimizer': 'Adam', 'learning_rate': 0.00028621929061275363, 'num_epochs': 25}. Best is trial 5 with value: 0.7278563711287219.
[I 2025-02-26 12:17:14,282] Trial 6 finished with value: 0.7625103381876817 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.004310947072687491, 'num_epochs': 29}. Best is trial 6 with value: 0.7625103381876817.
[I 2025-02-26 12:17:15,305] Trial 7 finished with value: 0.5243908839354064 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.0001536782512889755, 'num_epochs': 18}. Best is trial 6 with value: 0.7625103381876817.
[I 2025-02-26 12:17:15,767] Trial 8 finished with value: 0.5042320966116081 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 1.9735333309485084e-05, 'num_epochs': 5}. Best is trial 6 with value: 0.7625103381876817.
[I 2025-02-26 12:17:17,246] Trial 9 finished with value: 0.7530688808334978 and parameters: {'model': 'MLPModel', 'optimizer': 'AdamW', 'learning_rate': 0.006459613145155783, 'num_epochs': 22}. Best is trial 6 with value: 0.7625103381876817.
[I 2025-02-26 12:17:18,754] Trial 10 finished with value: 0.7392433080028884 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'Adam', 'learning_rate': 0.0015459676379730998, 'num_epochs': 30}. Best is trial 6 with value: 0.7625103381876817.
[I 2025-02-26 12:17:20,871] Trial 11 finished with value: 0.7546379326882183 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 0.009911618864379712, 'num_epochs': 30}. Best is trial 6 with value: 0.7625103381876817.
[I 2025-02-26 12:17:23,097] Trial 12 finished with value: 0.7337081925499898 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 0.001099014191073543, 'num_epochs': 30}. Best is trial 6 with value: 0.7625103381876817.
[I 2025-02-26 12:17:24,442] Trial 13 finished with value: 0.7246829773256624 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 0.008750417866284863, 'num_epochs': 26}. Best is trial 6 with value: 0.7625103381876817.
[I 2025-02-26 12:17:26,420] Trial 14 finished with value: 0.7542596952745659 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 0.0019036265865256458, 'num_epochs': 26}. Best is trial 6 with value: 0.7625103381876817.
[I 2025-02-26 12:17:28,117] Trial 15 finished with value: 0.7559988634780107 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.0028380055561306023, 'num_epochs': 30}. Best is trial 6 with value: 0.7625103381876817.
[I 2025-02-26 12:17:28,918] Trial 16 finished with value: 0.7023850674040788 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.0009754587513041092, 'num_epochs': 13}. Best is trial 6 with value: 0.7625103381876817.
[I 2025-02-26 12:17:30,153] Trial 17 finished with value: 0.7557835034195196 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.0031228705275351313, 'num_epochs': 22}. Best is trial 6 with value: 0.7625103381876817.
[I 2025-02-26 12:17:31,491] Trial 18 finished with value: 0.6337612181778367 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.000528576792286561, 'num_epochs': 26}. Best is trial 6 with value: 0.7625103381876817.
[I 2025-02-26 12:17:32,756] Trial 19 finished with value: 0.7081273990477104 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.0037438810607945865, 'num_epochs': 24}. Best is trial 6 with value: 0.7625103381876817.
