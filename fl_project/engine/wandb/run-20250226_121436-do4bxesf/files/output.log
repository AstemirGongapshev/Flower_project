[I 2025-02-26 12:14:39,711] A new study created in memory with name: no-name-cb2dedf2-032b-4e0e-8b27-94b7ff88a9f7
[I 2025-02-26 12:14:40,779] Trial 0 finished with value: 0.5123099085534139 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'Adam', 'learning_rate': 0.0004916513328344318, 'num_epochs': 6}. Best is trial 0 with value: 0.5123099085534139.
[I 2025-02-26 12:14:42,797] Trial 1 finished with value: 0.5605333690457016 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 8.071655088682772e-05, 'num_epochs': 15}. Best is trial 1 with value: 0.5605333690457016.
[I 2025-02-26 12:14:45,519] Trial 2 finished with value: 0.768288864799127 and parameters: {'model': 'MLPModel', 'optimizer': 'RMSProp', 'learning_rate': 0.003280473578923079, 'num_epochs': 15}. Best is trial 2 with value: 0.768288864799127.
[I 2025-02-26 12:14:48,257] Trial 3 finished with value: 0.7712532326630628 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 0.008629328660461521, 'num_epochs': 24}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:14:50,100] Trial 4 finished with value: 0.7540407157192934 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 0.002393825795215502, 'num_epochs': 12}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:14:52,471] Trial 5 finished with value: 0.7677966132368617 and parameters: {'model': 'MLPModel', 'optimizer': 'RMSProp', 'learning_rate': 0.007983945369234418, 'num_epochs': 13}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:14:54,023] Trial 6 finished with value: 0.6126468837037586 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'Adam', 'learning_rate': 0.00014075104491146335, 'num_epochs': 11}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:14:55,270] Trial 7 finished with value: 0.5464444778242481 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'Adam', 'learning_rate': 2.744277378528288e-05, 'num_epochs': 5}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:14:58,871] Trial 8 finished with value: 0.5706724844044933 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'Adam', 'learning_rate': 1.0711764359655571e-05, 'num_epochs': 21}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:15:00,358] Trial 9 finished with value: 0.5416287373566452 and parameters: {'model': 'MLPModel', 'optimizer': 'AdamW', 'learning_rate': 4.989243360213777e-05, 'num_epochs': 3}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:15:05,598] Trial 10 finished with value: 0.7588510269417245 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 0.000647152216843317, 'num_epochs': 30}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:15:09,135] Trial 11 finished with value: 0.7667451494218759 and parameters: {'model': 'MLPModel', 'optimizer': 'RMSProp', 'learning_rate': 0.008181533787163468, 'num_epochs': 22}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:15:12,303] Trial 12 finished with value: 0.7672609277132201 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 0.002003816747236128, 'num_epochs': 21}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:15:16,805] Trial 13 finished with value: 0.7634658853379614 and parameters: {'model': 'MLPModel', 'optimizer': 'AdamW', 'learning_rate': 0.0019033741873347746, 'num_epochs': 27}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:15:19,104] Trial 14 finished with value: 0.7551519012311717 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.004537335162379172, 'num_epochs': 18}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:15:23,088] Trial 15 finished with value: 0.7627980881818 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 0.0011437291480821152, 'num_epochs': 25}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:15:25,485] Trial 16 finished with value: 0.7638278350160976 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.004390823987588492, 'num_epochs': 18}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:15:27,462] Trial 17 finished with value: 0.7241979647569599 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 0.0002203357387812257, 'num_epochs': 9}. Best is trial 3 with value: 0.7712532326630628.
[I 2025-02-26 12:15:31,670] Trial 18 finished with value: 0.7723101257232207 and parameters: {'model': 'MLPModel', 'optimizer': 'AdamW', 'learning_rate': 0.009927686915514192, 'num_epochs': 25}. Best is trial 18 with value: 0.7723101257232207.
[I 2025-02-26 12:15:35,153] Trial 19 finished with value: 0.7742700832303285 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'AdamW', 'learning_rate': 0.008881680494715579, 'num_epochs': 30}. Best is trial 19 with value: 0.7742700832303285.
