[I 2025-02-26 12:17:35,785] A new study created in memory with name: no-name-bb4ffb03-bde4-4278-97a4-f23618b48004
[I 2025-02-26 12:17:36,722] Trial 0 finished with value: 0.6067832989179515 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'AdamW', 'learning_rate': 0.00011418238972688363, 'num_epochs': 20}. Best is trial 0 with value: 0.6067832989179515.
[I 2025-02-26 12:17:37,589] Trial 1 finished with value: 0.4405316678822143 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 3.187224352295636e-05, 'num_epochs': 13}. Best is trial 0 with value: 0.6067832989179515.
[I 2025-02-26 12:17:38,640] Trial 2 finished with value: 0.7259154159797164 and parameters: {'model': 'MLPModel', 'optimizer': 'AdamW', 'learning_rate': 0.0002744810388787323, 'num_epochs': 17}. Best is trial 2 with value: 0.7259154159797164.
[I 2025-02-26 12:17:39,635] Trial 3 finished with value: 0.6483495999551182 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.00020639973877690284, 'num_epochs': 21}. Best is trial 2 with value: 0.7259154159797164.
[I 2025-02-26 12:17:40,368] Trial 4 finished with value: 0.5005347806494463 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'Adam', 'learning_rate': 2.4728150456489638e-05, 'num_epochs': 14}. Best is trial 2 with value: 0.7259154159797164.
[I 2025-02-26 12:17:41,078] Trial 5 finished with value: 0.4268572090422269 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 2.3713661788331192e-05, 'num_epochs': 14}. Best is trial 2 with value: 0.7259154159797164.
[I 2025-02-26 12:17:42,055] Trial 6 finished with value: 0.7726304511883713 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'AdamW', 'learning_rate': 0.007194008892020931, 'num_epochs': 22}. Best is trial 6 with value: 0.7726304511883713.
[I 2025-02-26 12:17:43,053] Trial 7 finished with value: 0.7280509190807202 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 0.00013980982869765525, 'num_epochs': 16}. Best is trial 6 with value: 0.7726304511883713.
[I 2025-02-26 12:17:44,640] Trial 8 finished with value: 0.6173639928840692 and parameters: {'model': 'MLPModel', 'optimizer': 'RMSProp', 'learning_rate': 1.5795139674523392e-05, 'num_epochs': 27}. Best is trial 6 with value: 0.7726304511883713.
[I 2025-02-26 12:17:45,586] Trial 9 finished with value: 0.7558196983873331 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 0.008821816914500756, 'num_epochs': 15}. Best is trial 6 with value: 0.7726304511883713.
[I 2025-02-26 12:17:45,923] Trial 10 finished with value: 0.6388828061234646 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'AdamW', 'learning_rate': 0.006636369605919986, 'num_epochs': 3}. Best is trial 6 with value: 0.7726304511883713.
[I 2025-02-26 12:17:47,521] Trial 11 finished with value: 0.7591984986327351 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 0.00923388855979608, 'num_epochs': 27}. Best is trial 6 with value: 0.7726304511883713.
[I 2025-02-26 12:17:49,198] Trial 12 finished with value: 0.7607738846068232 and parameters: {'model': 'MLPModel', 'optimizer': 'Adam', 'learning_rate': 0.0016955093399176032, 'num_epochs': 30}. Best is trial 6 with value: 0.7726304511883713.
[I 2025-02-26 12:17:50,398] Trial 13 finished with value: 0.7542162613131895 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'Adam', 'learning_rate': 0.0018937520514378727, 'num_epochs': 29}. Best is trial 6 with value: 0.7726304511883713.
[I 2025-02-26 12:17:51,436] Trial 14 finished with value: 0.7024375501074085 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'Adam', 'learning_rate': 0.001142043479961239, 'num_epochs': 24}. Best is trial 6 with value: 0.7726304511883713.
[I 2025-02-26 12:17:53,125] Trial 15 finished with value: 0.7537556803477614 and parameters: {'model': 'MLPModel', 'optimizer': 'Adam', 'learning_rate': 0.002073820933310445, 'num_epochs': 30}. Best is trial 6 with value: 0.7726304511883713.
[I 2025-02-26 12:17:53,691] Trial 16 finished with value: 0.6540810731084058 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'AdamW', 'learning_rate': 0.0006886873297125928, 'num_epochs': 9}. Best is trial 6 with value: 0.7726304511883713.
[I 2025-02-26 12:17:55,111] Trial 17 finished with value: 0.7397283205715909 and parameters: {'model': 'MLPModel', 'optimizer': 'AdamW', 'learning_rate': 0.004051091846396222, 'num_epochs': 23}. Best is trial 6 with value: 0.7726304511883713.
[I 2025-02-26 12:17:56,683] Trial 18 finished with value: 0.7387067176050515 and parameters: {'model': 'MLPModel', 'optimizer': 'Adam', 'learning_rate': 0.000597141669230618, 'num_epochs': 25}. Best is trial 6 with value: 0.7726304511883713.
[I 2025-02-26 12:17:57,782] Trial 19 finished with value: 0.7592093571230791 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'AdamW', 'learning_rate': 0.0030942574024516007, 'num_epochs': 20}. Best is trial 6 with value: 0.7726304511883713.
