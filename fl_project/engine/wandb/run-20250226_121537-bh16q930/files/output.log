[I 2025-02-26 12:15:38,117] A new study created in memory with name: no-name-30e47f21-6aa2-4bf7-aa60-9eadc92efd35
[I 2025-02-26 12:15:40,847] Trial 0 finished with value: 0.6679021215680384 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'AdamW', 'learning_rate': 4.973377267024452e-05, 'num_epochs': 30}. Best is trial 0 with value: 0.6679021215680384.
[I 2025-02-26 12:15:43,834] Trial 1 finished with value: 0.5556398093972995 and parameters: {'model': 'MLPModel', 'optimizer': 'AdamW', 'learning_rate': 2.6725634375054785e-05, 'num_epochs': 25}. Best is trial 0 with value: 0.6679021215680384.
[I 2025-02-26 12:15:47,018] Trial 2 finished with value: 0.7783673535868308 and parameters: {'model': 'MLPModel', 'optimizer': 'RMSProp', 'learning_rate': 0.006494110599091357, 'num_epochs': 28}. Best is trial 2 with value: 0.7783673535868308.
[I 2025-02-26 12:15:50,459] Trial 3 finished with value: 0.6543425817508592 and parameters: {'model': 'MLPModel', 'optimizer': 'AdamW', 'learning_rate': 5.4059363437757115e-05, 'num_epochs': 28}. Best is trial 2 with value: 0.7783673535868308.
[I 2025-02-26 12:15:51,993] Trial 4 finished with value: 0.38745265245772875 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'Adam', 'learning_rate': 2.7310427203257982e-05, 'num_epochs': 11}. Best is trial 2 with value: 0.7783673535868308.
[I 2025-02-26 12:15:54,960] Trial 5 finished with value: 0.6557677586085207 and parameters: {'model': 'MLPModel', 'optimizer': 'AdamW', 'learning_rate': 2.8169086507062267e-05, 'num_epochs': 22}. Best is trial 2 with value: 0.7783673535868308.
[I 2025-02-26 12:15:57,564] Trial 6 finished with value: 0.5553665373903067 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 2.762491549311207e-05, 'num_epochs': 30}. Best is trial 2 with value: 0.7783673535868308.
[I 2025-02-26 12:15:59,417] Trial 7 finished with value: 0.7617031904054379 and parameters: {'model': 'MLPModel', 'optimizer': 'RMSProp', 'learning_rate': 0.0013582872157978168, 'num_epochs': 13}. Best is trial 2 with value: 0.7783673535868308.
[I 2025-02-26 12:16:00,819] Trial 8 finished with value: 0.6141607382325636 and parameters: {'model': 'MLPModel', 'optimizer': 'SGD', 'learning_rate': 4.2894670535004874e-05, 'num_epochs': 9}. Best is trial 2 with value: 0.7783673535868308.
[I 2025-02-26 12:16:02,850] Trial 9 finished with value: 0.678702699963624 and parameters: {'model': 'MLPModel', 'optimizer': 'RMSProp', 'learning_rate': 0.0001030033905167804, 'num_epochs': 16}. Best is trial 2 with value: 0.7783673535868308.
[I 2025-02-26 12:16:04,835] Trial 10 finished with value: 0.7805100956813973 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.008505516126507918, 'num_epochs': 21}. Best is trial 10 with value: 0.7805100956813973.
[I 2025-02-26 12:16:06,797] Trial 11 finished with value: 0.7810150154823975 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.00893945590709991, 'num_epochs': 21}. Best is trial 11 with value: 0.7810150154823975.
[I 2025-02-26 12:16:07,531] Trial 12 finished with value: 0.7416131735204854 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.009043397219082911, 'num_epochs': 3}. Best is trial 11 with value: 0.7810150154823975.
[I 2025-02-26 12:16:09,595] Trial 13 finished with value: 0.7587804467544879 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.0023547225689819536, 'num_epochs': 21}. Best is trial 11 with value: 0.7810150154823975.
[I 2025-02-26 12:16:11,572] Trial 14 finished with value: 0.7201079333940201 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'Adam', 'learning_rate': 0.00057788671353315, 'num_epochs': 18}. Best is trial 11 with value: 0.7810150154823975.
[I 2025-02-26 12:16:13,650] Trial 15 finished with value: 0.768806452838862 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.003262016942201206, 'num_epochs': 22}. Best is trial 11 with value: 0.7810150154823975.
[I 2025-02-26 12:16:15,456] Trial 16 finished with value: 0.5665553068156934 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.00030829992590821354, 'num_epochs': 18}. Best is trial 11 with value: 0.7810150154823975.
[I 2025-02-26 12:16:17,075] Trial 17 finished with value: 0.7075935232724594 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'RMSProp', 'learning_rate': 0.0007747272943363918, 'num_epochs': 15}. Best is trial 11 with value: 0.7810150154823975.
[I 2025-02-26 12:16:19,346] Trial 18 finished with value: 0.7714106807730521 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'Adam', 'learning_rate': 0.0040969365653138225, 'num_epochs': 25}. Best is trial 11 with value: 0.7810150154823975.
[I 2025-02-26 12:16:21,316] Trial 19 finished with value: 0.7768163992160171 and parameters: {'model': 'LogisticRegressionModel', 'optimizer': 'SGD', 'learning_rate': 0.00993791103895016, 'num_epochs': 20}. Best is trial 11 with value: 0.7810150154823975.
